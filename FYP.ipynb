{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import datetime\n",
    "import time;\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import io\n",
    "\n",
    "# iccid,imei,imsi, mac address, android id, android advertiser id, device_id\n",
    "# name , gender, dob, relation ship status, email address, phone number, address book infromation\n",
    "# GPS lattitude longitude, zip code\n",
    "# username, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (randomForest.py, line 158)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2963\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-184-658db98fcab4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import randomForest as rf\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Junaid\\Desktop\\ReCon\\My Code\\randomForest.py\"\u001b[1;36m, line \u001b[1;32m158\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import randomForest as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with io.open(os.path.join(\"./\", 'glove.6B.100d.txt'), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    \n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.d0 = defaultdict(lambda:0)\n",
    "        self.d1 = defaultdict(lambda:0)\n",
    "        self.d = defaultdict(lambda:0)\n",
    "        self.prior0 = 0.0\n",
    "        self.prior1 = 0.0\n",
    "        self.c0 = 0\n",
    "        self.c1 = 0\n",
    "    def train(self,text,label):\n",
    "        \n",
    "        self.prior0 = len(label[label==0]) / len(label)\n",
    "        self.prior1 = len(label[label==1]) / len(label)  \n",
    "        #print(self.prior0,'\\n',self.prior1)\n",
    "        \n",
    "        i=0\n",
    "        for t in text:\n",
    "            if label[i]==0:\n",
    "                for w in t:\n",
    "                    self.d0[w]+=1\n",
    "                    self.d[w]+=1\n",
    "                    self.c0+=1\n",
    "            else:\n",
    "                for w in t:\n",
    "                    self.d1[w]+=1\n",
    "                    self.d[w]+=1\n",
    "                    self.c1+=1\n",
    "            i+=1\n",
    "                    \n",
    "    def test(self,texts):\n",
    "        \n",
    "        l = []\n",
    "        vocab = len(self.d)\n",
    "        for words in texts:\n",
    "            #print(\"\\n\\npacket:\\n\",words)\n",
    "            L0 = 0.0\n",
    "            L1 = 0.0\n",
    "            for w in words:\n",
    "                #print (\"WORD: \",w)\n",
    "                L0 += math.log( (self.d0[w]+1) / ( self.c0 + vocab ) )\n",
    "                L1 += math.log( (self.d1[w]+1) / ( self.c1 + vocab ) )\n",
    "                #print(\"0: \",self.d0[w],L0)\n",
    "                #print('1: ',self.d1[w],L1)\n",
    "            #print('likeli: ',L0,'  ',L1)\n",
    "            p0 = L0#self.prior0 * L0\n",
    "            p1 = L1#self.prior1 * L1\n",
    "            #print(\"Final prob: \",p0,'  ',p1)\n",
    "            if p0>=p1:\n",
    "                l.append(0)\n",
    "            else:\n",
    "                l.append(1)\n",
    "            #a=input(\"waiting...\")\n",
    "        return np.array(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data and storing inside pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets = None\n",
    "i=0\n",
    "total=0\n",
    "for filename in os.listdir('android/'):\n",
    "    \n",
    "    #print('============================'+filename+'============================')\n",
    "    d = pd.read_json('android/'+filename)\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    df=df.T\n",
    "    total+=len(df)\n",
    "    if i==0:\n",
    "        packets=df\n",
    "    else:\n",
    "        packets=packets.append(df)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_domains=(np.unique((packets.domain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "common_domains = []\n",
    "num_packets = []\n",
    "for u in unique_domains:\n",
    "    num_of_packets = len(packets.domain[packets.domain==u])\n",
    "    if num_of_packets>150:\n",
    "        common_domains.append(u)\n",
    "        num_packets.append(num_of_packets)\n",
    "common_domains = np.array(common_domains)\n",
    "num_packets = np.array(num_packets)\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(common_domains, num_packets, align='center',\n",
    "        color='green', ecolor='black')\n",
    "ax.set_xlabel('Number of Packets')\n",
    "ax.set_title('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udomains_leaking_pii = (np.unique(packets.domain[packets.o_pii == 1]))\n",
    "domains_leaking_pii = ((packets.domain[packets.o_pii == 1]))\n",
    "common_domains_leaking_pii = []\n",
    "count_of_packets = []\n",
    "for u in udomains_leaking_pii:\n",
    "    num_of_packets = len(domains_leaking_pii[domains_leaking_pii==u])\n",
    "    if num_of_packets>20:\n",
    "        common_domains_leaking_pii.append(u)\n",
    "        count_of_packets.append(num_of_packets)\n",
    "        \n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(common_domains_leaking_pii, count_of_packets, align='center',color='green', ecolor='black')\n",
    "ax.set_xlabel('Number of Personal Infromation Leaks')\n",
    "ax.set_title('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piid = defaultdict(lambda:0)\n",
    "piis = packets.pii_types[packets.o_pii==1]\n",
    "piis[0][0]\n",
    "for i in piis:\n",
    "    for j in i:\n",
    "        piid[j.lower()]+=1\n",
    "        \n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh((list(piid.keys())), (list(piid.values())), align='center',color='#539caf', ecolor='black')\n",
    "ax.set_xlabel('Number of Personal Infromation Leaks')\n",
    "ax.set_title('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning labels to data packets\n",
    "# Using String Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri in packets.uri:\n",
    "    uri=uri.lower()\n",
    "    token = re.split('&',uri)\n",
    "    chk=re.findall('mac.?address|imei|iccid|imsi|android.?id|advertiser.?id|gender|latitude|longitude|zip.?code|user.?name|password|email|firstname|macaddr|serialnumber',uri)\n",
    "    chk=np.array(chk)\n",
    "    chk2=0\n",
    "    if (len(chk)>0):\n",
    "        chk2=0\n",
    "        for pi in np.unique(chk):\n",
    "            for t in token:\n",
    "                ind=t.find(pi)+len(pi)\n",
    "                if ind+8 < len(t):\n",
    "                    chk2=1\n",
    "                    break\n",
    "            if chk2==1:\n",
    "                break\n",
    "    if (chk2==1):\n",
    "        sm_label.append(1)\n",
    "    else:\n",
    "        sm_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_label=np.array(sm_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(packets)):\n",
    "    row=packets.iloc[i]\n",
    "    if (row.pii_types==None):\n",
    "        o_label.append(0)\n",
    "    else:\n",
    "        o_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_label=np.array(o_label)\n",
    "packets['o_pii']=o_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringMatching_cm = confusion_matrix(o_label,sm_label)\n",
    "stringMatching_cm = pandas.DataFrame(stringMatching_cm, ['Non-PII','PII'], ['Non-PII','PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringMatching_acc = (len(o_label[o_label==sm_label])/len(o_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringMatching_f1 = f1_score(o_label,sm_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Words From Data Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(raw_data):\n",
    "    wrds = []\n",
    "    tokens = re.split(\",|\\t|/|\\\\||\\\\*|!|#|&|\\\\?|\\n|;|\\\\{|\\\\}|\\\\(|\\\\)| \",raw_data)\n",
    "    #print(\"BEFORE ::::::::: \",tokens)\n",
    "    for i in range(0,len(tokens)):\n",
    "        t = tokens[i]\n",
    "        t=t.strip()\n",
    "        #print (\"TOKEN UTHAYA HA: \",t)\n",
    "        if (len(t) < 1):\n",
    "            continue\n",
    "        t=re.sub(\"\\\"|\\'|\\\\[|\\\\]|http://|https://|ftp://\",\"\",t)\n",
    "        isval = True\n",
    "        if (':' in t and '=' not in t):\n",
    "            t.replace(':', '=', 1)\n",
    "        if ('=' in t):\n",
    "            tsplit = re.split('=',t)\n",
    "            #print (\"SPLITS HAIN: \", tsplit)\n",
    "            if len(tsplit)>1 and '' not in tsplit:\n",
    "                isval = False\n",
    "                reconk = tsplit[0]\n",
    "                reconv = tsplit[1].strip()\n",
    "                #print(\"P YA HA:\",reconv)\n",
    "                if (reconv[-1]==':' or reconv[-1]=='-' or reconv[-1]=='='):\n",
    "                    reconv = reconv[0:len(reconv)]\n",
    "                #print(\"YA HA:\",reconv)\n",
    "                if (reconv[0]=='>'):\n",
    "                    reconv = reconv[1:]\n",
    "                if len(reconk)>=1:\n",
    "                    wrds.append(reconk)\n",
    "                if len(reconv)>=1:\n",
    "                    wrds.append(reconv)\n",
    "            elif len(tsplit) == 1:\n",
    "                t = tsplit[0]\n",
    "        if (isval):\n",
    "            t=t.strip()\n",
    "            if (len(t)==0):\n",
    "                continue\n",
    "            if len(t) == 1:\n",
    "                tmpt = t[0]\n",
    "                if (tmpt.isdigit==False and tmpt.isalpha==False):\n",
    "                    continue\n",
    "            wrds.append(t)\n",
    "        tokens[i]=t\n",
    "    return wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "tokenize_packets = []\n",
    "label = []\n",
    "for i in range(0,len(packets)):\n",
    "    row=packets.iloc[i]\n",
    "    words = []\n",
    "    words = text_to_words(row.uri.lower())\n",
    "    text.extend(words)\n",
    "    tokenize_packets.append(words)\n",
    "    label.append(row.o_pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_packets = np.array(tokenize_packets)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pii = defaultdict(lambda:0)\n",
    "dic_nonpii = defaultdict(lambda:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for p in tokenize_packets:\n",
    "    if (label[i]==0):\n",
    "        for w in p:\n",
    "            dic_nonpii[w]+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.train(tokenize_packets[0:9800],label[0:9800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = nb.test(tokenize_packets[9800:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes_cm = confusion_matrix(label[9800:],pl)\n",
    "naiveBayes_cm = pandas.DataFrame(naiveBayes_cm, ['Non-PII','PII'], ['Non-PII','PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otlabel = label[9800:]\n",
    "naiveBayes_acc = len(otlabel[pl == otlabel])/len(otlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes_f1 = f1_score(otlabel, pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(lambda:0)\n",
    "for w in text:\n",
    "    d[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = []\n",
    "for k in d.keys():\n",
    "    if d[k]>1:\n",
    "        good_words.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6853"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "c=0\n",
    "for w in good_words:\n",
    "    f = []\n",
    "    for p in tokenize_packets:\n",
    "        if w in p:\n",
    "            f.append(1)\n",
    "        else:\n",
    "            f.append(0)\n",
    "    features.append(f)\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = np.array(features)\n",
    "nf = nf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenize_packets, label, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9809,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "c=0\n",
    "for w in good_words:\n",
    "    f = []\n",
    "    for p in X_train:\n",
    "        if w in p:\n",
    "            f.append(1)\n",
    "        else:\n",
    "            f.append(0)\n",
    "    train_features.append(f)\n",
    "    c+=1\n",
    "train_features = np.array(train_features)\n",
    "train_features = train_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "c=0\n",
    "for w in good_words:\n",
    "    f = []\n",
    "    for p in X_test:\n",
    "        if w in p:\n",
    "            f.append(1)\n",
    "        else:\n",
    "            f.append(0)\n",
    "    test_features.append(f)\n",
    "    c+=1\n",
    "test_features = np.array(test_features)\n",
    "test_features = test_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraTrees_acc = len(y_test[y_test==predictions]) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraTrees_cm = confusion_matrix(y_test, predictions)\n",
    "extraTrees_cm = pandas.DataFrame(extraTrees_cm, ['Non-PII','PII'], ['Non-PII','PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraTrees_f1 = f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9568345323741008"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTrees_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-PII</th>\n",
       "      <th>PII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-PII</th>\n",
       "      <td>2835</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PII</th>\n",
       "      <td>27</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Non-PII  PII\n",
       "Non-PII     2835    9\n",
       "PII           27  399"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTrees_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_packets = X_test[predictions==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting PIIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "piis_in_packets = []\n",
    "for i in range(len(pii_packets)):\n",
    "    piis = []\n",
    "    for w in pii_packets[i]:\n",
    "        prob = 0.0\n",
    "        #print(w)\n",
    "        prob = dic_nonpii[w] / d[w]\n",
    "        if (prob<=0.0):\n",
    "            piis.append(w)\n",
    "    piis_in_packets.append(piis)\n",
    "        #print(prob)\n",
    "        #a=input(\"RUK JA O DIL DIWANAY\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5a666ad479ce6ffa5636b67dae7a2e55', 'design']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piis_in_packets[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfada = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preada=clfada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_cm = confusion_matrix(y_test, preada)\n",
    "ada_cm = pandas.DataFrame(ada_cm, ['Non-PII','PII'], ['Non-PII','PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_f1 = f1_score(y_test, preada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_acc = len(y_test[y_test==preada]) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted DTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfgtb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pregtb = clfgtb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_cm = confusion_matrix(y_test, pregtb)\n",
    "gbdt_cm = pandas.DataFrame(gbdt_cm, ['Non-PII','PII'], ['Non-PII','PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_f1 = f1_score(y_test, pregtb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_acc = len(y_test[y_test==pregtb]) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [naiveBayes_acc,stringMatching_acc,extraTrees_acc,ada_acc,gbdt_acc]\n",
    "l2 = [naiveBayes_f1,stringMatching_f1,extraTrees_f1,ada_f1,gbdt_f1]\n",
    "l3=[]\n",
    "l3.append(l1)\n",
    "l3.append(l2)\n",
    "l3=np.array(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = pandas.DataFrame(l3,['Accuracy','F1-measure'],['Naive Bayes','String Matching','Extra Trees',\"ADA Boosted Classifier\",\"Gradient Boosted Decsion Tress\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[1m' + 'Confusion Matrix Naive Bayes')\n",
    "naiveBayes_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[1m' + 'Confusion Matrix String Matching')\n",
    "stringMatching_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[1m' + 'Confusion Matrix Extra Trees')\n",
    "extraTrees_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[1m' + 'Confusion Matrix GBDT')\n",
    "gbdt_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\033[1m' + 'Confusion Matrix Ada Boosted Classifier')\n",
    "ada_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with monkey Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpackets = None\n",
    "i=0\n",
    "total=0\n",
    "for filename in os.listdir('monkey/'):\n",
    "    \n",
    "    #print filename+'============================')\n",
    "    d = pd.read_json('monkey/'+filename)\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    df=df.T\n",
    "    total+=len(df)\n",
    "    if i==0:\n",
    "        mpackets=df\n",
    "    else:\n",
    "        mpackets=mpackets.append(df,sort=False)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_label=[]\n",
    "for i in range(0,len(mpackets)):\n",
    "    row=mpackets.iloc[i]\n",
    "    if (row.pii_types==None):\n",
    "        mo_label.append(0)\n",
    "    else:\n",
    "        mo_label.append(1)\n",
    "mo_label=np.array(mo_label)\n",
    "mpackets['o_pii']=mo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtokenize_packets = []\n",
    "mlabel = []\n",
    "for i in range(0,len(mpackets)):\n",
    "    row=mpackets.iloc[i]\n",
    "    a=row.uri.lower()\n",
    "    a=re.sub('meid','imei',a)\n",
    "    words = text_to_words(a)\n",
    "    #print(words)\n",
    "    mtokenize_packets.append(words)\n",
    "    mlabel.append(row.o_pii)\n",
    "mlabel = np.array(mlabel)\n",
    "mtokenize_packets = np.array(mtokenize_packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfeatures = []\n",
    "for w in good_words:\n",
    "    f = []\n",
    "    for p in mtokenize_packets:\n",
    "        if w in p:\n",
    "            f.append(1)\n",
    "        else:\n",
    "            f.append(0)\n",
    "    mfeatures.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfeatures = np.array(mfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfeatures = mfeatures.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "mclf.fit(nf,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpre = mclf.predict(mfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpre = np.array(mpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(mlabel, mpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(mlabel, mpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piid = defaultdict(lambda:0)\n",
    "piis = mpackets.pii_types[mpackets.o_pii==1]\n",
    "for i in piis:\n",
    "    for j in i:\n",
    "        piid[j.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"Asd/''meidasldk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('meid','imei',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
